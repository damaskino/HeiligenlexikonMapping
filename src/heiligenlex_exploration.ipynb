{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784ad1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 85.8MB/s]\n",
      "2023-08-10 03:57:39 INFO: Downloading default packages for language: de (German) ...\n",
      "2023-08-10 03:57:41 INFO: File exists: C:\\Users\\chenz\\stanza_resources\\de\\default.zip\n",
      "2023-08-10 03:57:45 INFO: Finished downloading models and saved to C:\\Users\\chenz\\stanza_resources.\n",
      "2023-08-10 03:57:45 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 39.0MB/s]\n",
      "2023-08-10 03:57:46 INFO: Loading these models for language: de (German):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| pos       | gsd          |\n",
      "| lemma     | gsd          |\n",
      "| depparse  | gsd          |\n",
      "| sentiment | sb10k        |\n",
      "| ner       | germeval2014 |\n",
      "============================\n",
      "\n",
      "2023-08-10 03:57:46 INFO: Using device: cpu\n",
      "2023-08-10 03:57:46 INFO: Loading: tokenize\n",
      "2023-08-10 03:57:46 INFO: Loading: mwt\n",
      "2023-08-10 03:57:46 INFO: Loading: pos\n",
      "2023-08-10 03:57:47 INFO: Loading: lemma\n",
      "2023-08-10 03:57:47 INFO: Loading: depparse\n",
      "2023-08-10 03:57:47 INFO: Loading: sentiment\n",
      "2023-08-10 03:57:47 INFO: Loading: ner\n",
      "2023-08-10 03:57:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import stanza\n",
    "\n",
    "stanza.download('de')\n",
    "nlp = stanza.Pipeline('de')\n",
    "\n",
    "HLEX_SOUP_PICKLE = 'hlex_soup.pickle'\n",
    "occupation_list = []\n",
    "\n",
    "\n",
    "def load_transformed_hlex_to_soup():\n",
    "    hlex_xml_path = '../data/Heiligenlex-1858.xml'\n",
    "    with open(hlex_xml_path, 'r', encoding='utf-8') as hlex:\n",
    "        soup = BeautifulSoup(hlex, features=\"xml\")\n",
    "        return soup\n",
    "\n",
    "\n",
    "def pickle_it(object_to_pickle, path: str):\n",
    "    print(\"Attempting to pickle...\")\n",
    "    with open(path, 'wb') as target_file:\n",
    "        joblib.dump(value=object_to_pickle, filename=target_file)\n",
    "\n",
    "def timing_wrapper(func, param):\n",
    "    start = time.time()\n",
    "    value = None\n",
    "    if param:\n",
    "        value = func(param)\n",
    "    else:\n",
    "        print(\"no param found, running function without params\")\n",
    "        value = func()\n",
    "    end = time.time()\n",
    "    print(\"Finished after \", end - start)\n",
    "    return value\n",
    "\n",
    "\n",
    "def extract_occupation(paragraph_list):\n",
    "    occupation = None\n",
    "    # only look at the first two paragraphs for now\n",
    "\n",
    "    print(\"Full paragraph list\")\n",
    "    print(paragraph_list)\n",
    "\n",
    "    print(\"First paragraph\")\n",
    "    print(paragraph_list[0])\n",
    "    first_paragraph_text = paragraph_list[0].text\n",
    "\n",
    "    print(paragraph_list[1])\n",
    "\n",
    "    # for item in occupation_list:\n",
    "    #     if item.lower() in paragraph_text.lower():\n",
    "    #         occupation = item\n",
    "    #         continue\n",
    "    doc = nlp(first_paragraph_text)\n",
    "    print(doc)\n",
    "    doc.sentences[0].print_dependencies()\n",
    "\n",
    "    second_paragraph_text = paragraph_list[1].text\n",
    "    print(\"Second paragraph\")\n",
    "    print(second_paragraph_text)\n",
    "    doc2 = nlp(second_paragraph_text)\n",
    "    doc2.sentences[0].print_dependencies()\n",
    "    for sentence in doc2.sentences:\n",
    "        sentence.print_dependencies()\n",
    "    print(doc2.entities)\n",
    "\n",
    "    third_paragraph_text = paragraph_list[2].text\n",
    "    print(\"Third paragraph\")\n",
    "    print(third_paragraph_text)\n",
    "    # doc3 = nlp(third_paragraph_text)\n",
    "    # doc3.sentences[0].print_dependencies()\n",
    "\n",
    "    fourth_paragraph_text = paragraph_list[3].text\n",
    "    print(\"Fourth paragraph\")\n",
    "    print(fourth_paragraph_text)\n",
    "\n",
    "    sys.exit()\n",
    "    return occupation\n",
    "\n",
    "\n",
    "# the term of the entry contains the name of the saint and their title, usually one variant of: S., B., V. (Sanctus, Beati or Veritit\n",
    "def parse_term(term):\n",
    "    raw_term = term.text\n",
    "    print(\"Raw:\")\n",
    "    print(raw_term)\n",
    "\n",
    "    saint_name = None\n",
    "    canonization_status = None\n",
    "    hlex_number = None\n",
    "    footnote = None\n",
    "\n",
    "\n",
    "    import re\n",
    "\n",
    "    saint_pattern = r\"((\\w|\\s)+\\w)\\,?\\(?\"\n",
    "    canonization_pattern = r\"[A-Z]+\\.\"\n",
    "    number_pattern = r\"\\(.*\\)\"\n",
    "    footnote_pattern = r\"\\[.*\\]\"\n",
    "\n",
    "    saint_match = re.search(saint_pattern, raw_term)\n",
    "    if saint_match:\n",
    "        saint_name = saint_match.group(1)\n",
    "    else:\n",
    "        print(\"No match found for \", raw_term)\n",
    "        sys.exit()\n",
    "\n",
    "    canonization_match = re.search(canonization_pattern, raw_term)\n",
    "    if canonization_match:\n",
    "        canonization_status = canonization_match.group()\n",
    "\n",
    "    num_match = re.search(number_pattern, raw_term)\n",
    "    if num_match:\n",
    "        hlex_number = num_match.group()\n",
    "\n",
    "    footnote_match = re.search(footnote_pattern, raw_term)\n",
    "    if footnote_match:\n",
    "        footnote = footnote_match.group()\n",
    "\n",
    "    print(\"----------\")\n",
    "    print(saint_name)\n",
    "    if canonization_status:\n",
    "        print(canonization_status)\n",
    "    if hlex_number:\n",
    "        print(hlex_number)\n",
    "    if footnote:\n",
    "        print(footnote)\n",
    "        if hlex_number:\n",
    "            hlex_number = hlex_number + \" \" + footnote\n",
    "        else:\n",
    "            hlex_number = footnote\n",
    "    print(\"\\n\")\n",
    "    return saint_name, canonization_status, hlex_number\n",
    "\n",
    "\n",
    "# The paragraph contains free form text, but often starts with the feast day if it is available,\n",
    "# May also contain occupation of saint\n",
    "def parse_paragraph(paragraph_list):\n",
    "    paragraph = paragraph_list[0]\n",
    "    feast_day_pattern = r\"\\(.?[0-9][0-9]?.*?\\)\"\n",
    "    raw_paragraph = paragraph.text\n",
    "\n",
    "    feast_day = None\n",
    "\n",
    "    feast_day_match = re.search(feast_day_pattern, raw_paragraph)\n",
    "    if feast_day_match:\n",
    "        feast_day = feast_day_match.group()\n",
    "\n",
    "    occupation = extract_occupation(paragraph_list)\n",
    "    return feast_day, occupation\n",
    "\n",
    "def parse_entry(entry):\n",
    "    #namespace is found on linux, not in windows, maybe a module version error?\n",
    "    #term_list = entry.find_all('tei:term')\n",
    "    term_list = entry.find_all('term')\n",
    "    entry_id = entry.get('xml:id')\n",
    "\n",
    "    #TODO: This is more of a sanity check to verify an assumption about the data, would be nicer to move this to tests\n",
    "    if len(entry.find_all('sense'))>1:\n",
    "        print(\"Error: More than one sense found in entry!\")\n",
    "        sys.exit()\n",
    "\n",
    "    #print(term_list)\n",
    "    entry_dict = {}\n",
    "    #paragraph_list = entry.find_all('tei:p')\n",
    "    paragraph_list = entry.find_all('p')\n",
    "    #Assuming only one term per entry, give warning when finding other\n",
    "    print(\"Looking at entry: \", entry_id)\n",
    "    print(entry)\n",
    "    if len(term_list) > 1:\n",
    "        print(f\"Error, found more than one term in entry {entry_id}!\")\n",
    "        sys.exit()\n",
    "    else:\n",
    "        print(term_list)\n",
    "        term = term_list[0]\n",
    "        saint_name, canonization_status, hlex_number = parse_term(term)\n",
    "        entry_dict['SaintName'] = saint_name\n",
    "        entry_dict['CanonizationStatus'] = canonization_status\n",
    "        entry_dict['NumberInHlex'] = hlex_number\n",
    "        entry_dict['OriginalText'] = entry.text\n",
    "\n",
    "        #TODO looking only at first paragraph for now, will have to look at more later\n",
    "        if paragraph_list:\n",
    "\n",
    "            feast_day, occupation = parse_paragraph(paragraph_list)\n",
    "            entry_dict['FeastDay'] = feast_day\n",
    "            entry_dict['Ocupation'] = occupation\n",
    "        else:\n",
    "            entry_dict['FeastDay'] = None\n",
    "            entry_dict['Occupation'] = None\n",
    "\n",
    "        return entry_id, entry_dict\n",
    "\n",
    "def write_dict_to_json(data: dict):\n",
    "\n",
    "    json_data = json.dumps(data)\n",
    "    with open('tmp/parsed_heiligenlexikon.json', 'w') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "def parse_soup(soup):\n",
    "    entries = soup.find_all('entry')\n",
    "    data = {}\n",
    "    for e in entries[:]:\n",
    "        entry_id, entry_dict = parse_entry(e)\n",
    "\n",
    "        if entry_id in data.keys():\n",
    "            print(\"ERROR: Duplicate entry id found!\", entry_id)\n",
    "            sys.exit()\n",
    "\n",
    "        data[entry_id] = entry_dict\n",
    "\n",
    "    write_dict_to_json(data)\n",
    "\n",
    "\n",
    "def load_occupation_list():\n",
    "    with open(r\"occupation_list.txt\", \"r\") as occupation_file:\n",
    "        tmp_occupation_list = occupation_file.readlines()\n",
    "        for item in tmp_occupation_list:\n",
    "            if item.startswith('#'):\n",
    "                continue\n",
    "            occupation_list.append(item.strip())\n",
    "\n",
    "        print(occupation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hlex_soup = None\n",
    "load_occupation_list()\n",
    "#sys.setrecursionlimit(sys.getrecursionlimit()*50)\n",
    "#print(\"Attempting with recursionlimit:\", sys.getrecursionlimit())\n",
    "#TODO: Add a check to see if pickle file is corrupt\n",
    "if os.path.isfile('tmp/'+HLEX_SOUP_PICKLE):\n",
    "    print(\"Pickle found, loading...\")\n",
    "    with open('tmp/' + HLEX_SOUP_PICKLE, 'rb') as pickle_file:\n",
    "        hlex_soup = timing_wrapper(joblib.load, pickle_file)\n",
    "        #print(\"Hlex_soup is: \")\n",
    "        #print(hlex_soup)\n",
    "else:\n",
    "    print(\"No pickle found, loading from XML...\")\n",
    "    hlex_soup = timing_wrapper(load_transformed_hlex_to_soup, None)\n",
    "    print(\"Size of Hlex Object: \",sys.getsizeof(hlex_soup))\n",
    "    pickle_it(hlex_soup, \"tmp/\"+HLEX_SOUP_PICKLE)\n",
    "print(\"Loaded\", hlex_soup.title.text)\n",
    "parse_soup(hlex_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "048d5c9a-dfb9-45c5-bc2a-b439077cc80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_count_words',\n",
       " '_ents',\n",
       " '_lang',\n",
       " '_num_tokens',\n",
       " '_num_words',\n",
       " '_process_sentences',\n",
       " '_sentences',\n",
       " '_text',\n",
       " 'add_property',\n",
       " 'build_ents',\n",
       " 'entities',\n",
       " 'ents',\n",
       " 'from_serialized',\n",
       " 'get',\n",
       " 'get_mwt_expansions',\n",
       " 'iter_tokens',\n",
       " 'iter_words',\n",
       " 'lang',\n",
       " 'num_tokens',\n",
       " 'num_words',\n",
       " 'reindex_sentences',\n",
       " 'sentence_comments',\n",
       " 'sentences',\n",
       " 'set',\n",
       " 'set_mwt_expansions',\n",
       " 'text',\n",
       " 'to_dict',\n",
       " 'to_serialized']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e03a6cd2-e640-4f26-8c46-edd2fff9afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gender(input_name: str):\n",
    "    gender_pattern = re.compile(r\"Gender=(\\w+)\")\n",
    "    doc = nlp(input_name)\n",
    "    extracted_gender = None\n",
    "    print(doc)\n",
    "    feats = doc.get(\"feats\")\n",
    "    print(feats)\n",
    "    feats_str = feats[0]\n",
    "    if \"Gender\" in feats_str:\n",
    "        print(\"found gender\")\n",
    "        gender_match = re.search(gender_pattern, feats_str)\n",
    "        if gender_match:\n",
    "            extracted_gender = gender_match.group(1)\n",
    "    return extracted_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e004ffcf-0b10-4245-95b0-60ae7295b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Albericus\",\n",
      "      \"lemma\": \"Albericus\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NE\",\n",
      "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 9,\n",
      "      \"ner\": \"S-PER\",\n",
      "      \"multi_ner\": [\n",
      "        \"S-PER\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "]\n",
      "['Case=Nom|Gender=Masc|Number=Sing']\n",
      "found gender\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Masc'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_gender(\"Albericus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb19f8df-5972-45b8-80d2-5d61d8b8cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Anso\",\n",
      "      \"lemma\": \"anso\",\n",
      "      \"upos\": \"ADV\",\n",
      "      \"xpos\": \"ADV\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 4,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "]\n",
      "yay\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Anso\")\n",
    "print(doc)\n",
    "if doc.get(\"feats\"):\n",
    "    print(\"yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6aebc-e800-46c0-b00d-f01df8d52384",
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('it')\n",
    "nlp = stanza.Pipeline('de')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
